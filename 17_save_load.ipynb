{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17_save_load.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwekSQmJOqlJk8mnUrxp24",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z-arabi/pytorchTutorial/blob/master/17_save_load.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DtqdmNKv4hmm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' 3 DIFFERENT METHODS TO REMEMBER:\n",
        " - torch.save(arg, PATH) # can be model, tensor, or dictionary\n",
        " - torch.load(PATH)\n",
        " - torch.load_state_dict(arg)\n",
        "'''\n",
        "\n",
        "''' 2 DIFFERENT WAYS OF SAVING\n",
        "# 1) lazy way: save whole model\n",
        "torch.save(model, PATH)\n",
        "\n",
        "# model class must be defined somewhere\n",
        "model = torch.load(PATH)\n",
        "model.eval()\n",
        "\n",
        "# 2) recommended way: save only the state_dict\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# model must be created again with parameters\n",
        "model = Model(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "'''"
      ],
      "metadata": {
        "id": "giaQRIx04jNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "model = Model(n_input_features=6)\n",
        "# train your model..."
      ],
      "metadata": {
        "id": "8LRC0-1l4z9J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################save all ######################################\n",
        "for param in model.parameters():\n",
        "    print(param)\n",
        "\n",
        "# save and load entire model\n",
        "\n",
        "FILE = \"model.pth\"\n",
        "torch.save(model, FILE)\n",
        "\n",
        "loaded_model = torch.load(FILE)\n",
        "loaded_model.eval()\n",
        "\n",
        "for param in loaded_model.parameters():\n",
        "    print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo09NSvH46aD",
        "outputId": "43428dd2-6071-425c-aee3-51761c5f6a5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0890, -0.4039, -0.1319,  0.3017,  0.0116,  0.1560]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.3100], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0890, -0.4039, -0.1319,  0.3017,  0.0116,  0.1560]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.3100], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############save only state dict #########################\n",
        "\n",
        "# save only state dict\n",
        "FILE = \"model.pth\"\n",
        "torch.save(model.state_dict(), FILE)\n",
        "\n",
        "print(model.state_dict())\n",
        "loaded_model = Model(n_input_features=6)\n",
        "loaded_model.load_state_dict(torch.load(FILE)) # it takes the loaded dictionary, not the path file itself\n",
        "loaded_model.eval()\n",
        "\n",
        "print(loaded_model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWokRfp94_lS",
        "outputId": "20691d6c-c86c-43d8-b061-e820292ce6dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[-0.0890, -0.4039, -0.1319,  0.3017,  0.0116,  0.1560]])), ('linear.bias', tensor([-0.3100]))])\n",
            "OrderedDict([('linear.weight', tensor([[-0.0890, -0.4039, -0.1319,  0.3017,  0.0116,  0.1560]])), ('linear.bias', tensor([-0.3100]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########load checkpoint#####################\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "checkpoint = {\n",
        "\"epoch\": 90,\n",
        "\"model_state\": model.state_dict(),\n",
        "\"optim_state\": optimizer.state_dict()\n",
        "}\n",
        "print(optimizer.state_dict())\n",
        "FILE = \"checkpoint.pth\"\n",
        "torch.save(checkpoint, FILE)\n",
        "\n",
        "model = Model(n_input_features=6)\n",
        "optimizer = optimizer = torch.optim.SGD(model.parameters(), lr=0)\n",
        "\n",
        "checkpoint = torch.load(FILE)\n",
        "model.load_state_dict(checkpoint['model_state'])\n",
        "optimizer.load_state_dict(checkpoint['optim_state'])\n",
        "epoch = checkpoint['epoch']\n",
        "\n",
        "model.eval()\n",
        "# - or -\n",
        "# model.train()\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "# Remember that you must call model.eval() to set dropout and batch normalization layers \n",
        "# to evaluation mode before running inference. Failing to do this will yield \n",
        "# inconsistent inference results. If you wish to resuming training, \n",
        "# call model.train() to ensure these layers are in training mode.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMrdnk-F5DUg",
        "outputId": "292f5575-275c-4478-f6af-f901aafd076c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}\n",
            "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" SAVING ON GPU/CPU \n",
        "\n",
        "# 1) Save on GPU, Load on CPU\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "device = torch.device('cpu')\n",
        "model = Model(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH, map_location=device))\n",
        "\n",
        "# 2) Save on GPU, Load on GPU\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "model = Model(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.to(device)\n",
        "\n",
        "# Note: Be sure to use the .to(torch.device('cuda')) function \n",
        "# on all model inputs, too!\n",
        "\n",
        "# 3) Save on CPU, Load on GPU\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = Model(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))  # Choose whatever GPU device number you want\n",
        "model.to(device)\n",
        "\n",
        "# This loads the model to a given GPU device. \n",
        "# Next, be sure to call model.to(torch.device('cuda')) to convert the modelâ€™s parameter tensors to CUDA tensors\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IVRFGt1M5K5V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}